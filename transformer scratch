
import math
import numpy as np
class ANN():
    def __init__(self):
        self.weightdict = {}
        self.biasdict = {}
        self.activations = []
        self.neurons = []
        self.activationvalues = []
        self.layercount = 0
        self.initmethod = 'he'
        self.loss = 'mse'
        self.optimizer = 'adam'
        self.opt = Optimizer()

    def initializeweight(self, shape):
        fan_in = shape[0]
        fan_out = shape[1]

        if self.initmethod == 'he':
            limit = np.sqrt(2 / fan_in)
        elif self.initmethod == 'xavier':
            limit = np.sqrt(6 / (fan_in + fan_out))
        else:
            raise ValueError("Unsupported initialization method. Choose 'xavier' or 'he'.")
        return np.random.uniform(-limit, limit, size=shape)

    def add(self, neurons, activation, inputshape=None):
        if self.layercount == 0 and inputshape is not None:
            self.neurons.append(inputshape[1])

        self.neurons.append(neurons)
        self.activations.append(activation)

        self.weightdict[self.layercount] = self.initializeweight((self.neurons[-2], self.neurons[-1]))
        self.biasdict[self.layercount] = np.zeros((1, self.neurons[-1]))
        self.layercount += 1

    def activation(self, x, activation):
        if activation == 'linear':
            return x
        elif activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif activation == 'tanh':
            return np.tanh(x)
        elif activation == 'relu':
            return np.maximum(0, x)
        elif activation == 'leakyrelu':
            return np.maximum(0.01 * x, x)
        elif activation == 'softmax':
            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
            return exp_x / np.sum(exp_x, axis=1, keepdims=True)
        else:
            raise ValueError("Unsupported activation function.")

    def activationderivative(self, x, activation):
        if activation == 'linear':
            return np.ones_like(x)
        elif activation == 'sigmoid':
            sig = self.activation(x, 'sigmoid')
            return sig * (1 - sig)
        elif activation == 'tanh':
            return 1 - np.tanh(x) ** 2
        elif activation == 'relu':
            return np.where(x > 0, 1, 0)
        elif activation == 'leakyrelu':
            return np.where(x > 0, 1, 0.01)
        else:
            raise ValueError("Unsupported activation function.")

    def forward(self, x):
        self.activationvalues = [x]
        for i in range(self.layercount):
            x = np.dot(x, self.weightdict[i]) + self.biasdict[i]
            x = self.activation(x, self.activations[i])
            self.activationvalues.append(x)
        return x

    def lossderivative(self, y_true, y_pred):
        if self.loss == 'mse':
            return 2 * (y_pred - y_true)
        elif self.loss in ['binary_crossentropy', 'categorical_crossentropy']:
            return y_pred - y_true
        else:
            raise ValueError("Unsupported loss function.")

    def backward(self, x, y, z, learning_rate):
        delta = self.lossderivative(y, z)

        for j in range(self.layercount - 1, -1, -1):
            delta *= self.activationderivative(self.activationvalues[j + 1], self.activations[j])

            dw = np.dot(self.activationvalues[j].T, delta) / x.shape[0]
            db = np.sum(delta, axis=0, keepdims=True) / x.shape[0]

            self.weightdict[j], self.biasdict[j] = self.opt.optimizer(
                self.optimizer, self.weightdict[j], dw, self.biasdict[j], db, learning_rate
            )

            if j > 0:
                delta = np.dot(delta, self.weightdict[j].T)
    def backprop_from_output(self, dy):
    delta = dy.copy()
    for j in range(self.layercount - 1, -1, -1):
        delta *= self.activationderivative(self.activationvalues[j + 1], self.activations[j])
        dw = np.dot(self.activationvalues[j].T, delta) / delta.shape[0]
        db = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]

        self.weightdict[j], self.biasdict[j] = self.opt.optimizer(
            self.optimizer, self.weightdict[j], dw, self.biasdict[j], db, learning_rate=0.001
        )

        if j > 0:
            delta = np.dot(delta, self.weightdict[j].T)
    return delta  

    def fit(self, x, y, epochs, learning_rate):

        for _ in range(epochs):
            z = self.forward(x)
            self.backward(x, y, z, learning_rate)
        return self.weightdict, self.biasdict

    def predict(self, x):
        return self.forward(x)
class Optimizer():
    def __init__(self):
        self.m_w = 0
        self.v_w = 0
        self.m_b = 0
        self.v_b = 0
        self.t = 0
    def optimizer(self, optimizer, w, dw, b, db, learning_rate, beta1=0.9, beta2=0.99, eps=1e-8):
        self.t += 1
        if optimizer == 'adam':
            self.m_w = beta1 * self.m_w + (1 - beta1) * dw
            self.m_b = beta1 * self.m_b + (1 - beta1) * db
            self.v_w = beta2 * self.v_w + (1 - beta2) * dw ** 2
            self.v_b = beta2 * self.v_b + (1 - beta2) * db ** 2
            m_w_hat = self.m_w / (1 - beta1 ** self.t)
            m_b_hat = self.m_b / (1 - beta1 ** self.t)
            v_w_hat = self.v_w / (1 - beta2 ** self.t)
            v_b_hat = self.v_b / (1 - beta2 ** self.t)
            w -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + eps)
            b -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + eps)
        elif optimizer == 'rmsprop':
            self.v_w = beta1 * self.v_w + (1 - beta1) * dw ** 2
            self.v_b = beta1 * self.v_b + (1 - beta1) * db ** 2
            w -= learning_rate * dw / (np.sqrt(self.v_w) + eps)
            b -= learning_rate * db / (np.sqrt(self.v_b) + eps)
        elif optimizer == 'sgd':
            w -= learning_rate * dw
            b -= learning_rate * db
        elif optimizer == 'momentum':
            self.v_w = beta1 * self.v_w + learning_rate * dw
            self.v_b = beta1 * self.v_b + learning_rate * db
            w -= self.v_w
            b -= self.v_b
        else:
            raise ValueError("Unsupported optimizer function.")
        return w, b

class TransformerEncoder():
    def __init__(self, input_matrix):
        self.input_matrix = input_matrix
        self.d_model = input_matrix.shape[1]
        self.num_heads = 8
        self.d_k = self.d_model // self.num_heads
        self.d_v = self.d_k
        self.initialize_projection_weights()
        self.norm_stats = []
        self.ann = ANN()
        self.query=[]
        self.key=[]
        self.value=[]
        self.attn=[]

    def initialize_projection_weights(self):
        self.Wq_list = [self.init_weight((self.d_model, self.d_k)) for _ in range(self.num_heads)]
        self.Wk_list = [self.init_weight((self.d_model, self.d_k)) for _ in range(self.num_heads)]
        self.Wv_list = [self.init_weight((self.d_model, self.d_v)) for _ in range(self.num_heads)]

    def init_weight(self, shape):
        return np.random.randn(*shape) * np.sqrt(2 / shape[0])

    def positional_encoding(self, matrix):
        seq_len, d_model = matrix.shape
        pos_enc = np.zeros((seq_len, d_model))
        for pos in range(seq_len):
            for i in range(0, d_model, 2):
                angle = pos / (10000 ** ((2 * i) / d_model))
                pos_enc[pos, i] = np.sin(angle)
                if i + 1 < d_model:
                    pos_enc[pos, i + 1] = np.cos(angle)
        return pos_enc

    def layer_normalization(self, x, eps=1e-6):
        mean = np.mean(x, axis=1, keepdims=True)
        std = np.std(x, axis=1, keepdims=True)
        D = x.shape[1]
        self.norm_stats.append((x.copy(), mean, D, std))
        return (x - mean) / (std + eps)

    def layernorm_backward(self, dy, x, mean, D, sigma, eps=1e-6):
        x_centered = x - mean
        term1 = dy / (sigma + eps)
        mean_dy = np.mean(dy, axis=1, keepdims=True)
        mean_dy_xmu = np.mean(dy * x_centered, axis=1, keepdims=True)
        dx = (term1 -
              mean_dy / (sigma + eps) -
              x_centered * mean_dy_xmu / ((sigma + eps) ** 3))
        return dx

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    def single_head_attention(self, Q, K, V):
        scores = np.dot(Q, K.T) / np.sqrt(self.d_k)
        attention_weights = self.softmax(scores)
        return np.dot(attention_weights, V),attention_weights

    def multihead_attention(self, Q_input, K_input, V_input):
        Q_input += self.positional_encoding(Q_input)
        outputs = []
        for i in range(self.num_heads):
            Q = np.dot(Q_input, self.Wq_list[i])
            K = np.dot(K_input, self.Wk_list[i])
            V = np.dot(V_input, self.Wv_list[i])
            self.query.append(Q)
            self.key.append(K)
            self.value.append(V)
            head_output,attn_weights== self.single_head_attention(Q, K, V)
            self.attn.append(attn_weights)
            outputs.append(head_output)
        concatenated = np.concatenate(outputs, axis=1)
        residual = Q_input + concatenated
        return self.layer_normalization(residual)

    def feed_forward_network(self, x):
        self.ann.add(2048, "relu", inputshape=x.shape)
        self.ann.add(self.d_model, "linear")
        ffn_output = self.ann.forward(x)
        residual = x + ffn_output
        return self.layer_normalization(residual)

    def forward(self):
        attn_output = self.multihead_attention(self.input_matrix,
                                               self.input_matrix,
                                               self.input_matrix)
        return self.feed_forward_network(attn_output)
    def multiheadattentionbackward(self, delta):
        for h in range(self.heads):
            dels = delta[:,h*self.d_k:(h+1)*self.d_k]
            Q=self.query[h]
            K=self.key[h]
            V=self.value[h]
            A=self.attn[h]
            dV=np.dot(A.T,dels)
            dwv=np.dot(dV,self.input_matrix.T)
            d_scores=np.dot(dels,V.T)
            softmax_grad=np.diagflat(A)-np.dot(A,A.T)
            d_attn_scores=np.dot(softmax_grad,d_scores)/np.sqrt(self.d_k)
            dQ=np.dot(d_attn_scores,K)
            dK=np.dot(d_attn_scores.T,Q)
            dwq=np.dot(dQ,self.input_matrix.T)
            dwk=np.dot(dK,self.input_matrix.T)

            self.optimizer(self.Wq_list[h],dwq,learning_rate=0.001)
            self.optimizer(self.Wk_list[h],dwk,learning_rate=0.001)
            self.optimizer(self.Wv_list[h],dwv,learning_rate=0.001)
    def backward(self,delta):
        delta=self.layernorm_backward(delta,self.norm_stats[-1])
        delta=self.ann.backprop_from_output(delta)
        delta=self.layernorm_backward(delta,self.norm_stats[-2])
        self.multihead_attentionbackward(delta)
        return delta
class TransformerDecoder():
    def __init__(self, output_matrix, encoder_output):
        self.output_matrix = output_matrix
        self.encoder_output = encoder_output
        self.d_model = output_matrix.shape[1]
        self.num_heads = 8
        self.d_k = self.d_model // self.num_heads
        self.d_v = self.d_k
        self.initialize_decoder_weights()
        self.norm_stats = []
        self.ann = ANN()
        self.crossquery=[]
        self.crosskey=[]
        self.crossvalue=[]
        self.crossattn=[]
        self.maskquery=[]
        self.maskkey=[]
        self.maskvalue=[]
        self.maskattn=[]
    def init_weight(self, shape):
        return np.random.randn(*shape) * np.sqrt(2 / shape[0])

    def initialize_decoder_weights(self):
        self.wq_masked = [self.init_weight((self.d_model, self.d_k)) for _ in range(self.num_heads)]
        self.wk_masked = [self.init_weight((self.d_model, self.d_k)) for _ in range(self.num_heads)]
        self.wv_masked = [self.init_weight((self.d_model, self.d_v)) for _ in range(self.num_heads)]

        self.wq_cross = [self.init_weight((self.d_model, self.d_k)) for _ in range(self.num_heads)]
        self.wk_cross = [self.init_weight((self.d_model, self.d_k)) for _ in range(self.num_heads)]
        self.wv_cross = [self.init_weight((self.d_model, self.d_v)) for _ in range(self.num_heads)]

    def positional_encoding(self, matrix):
        seq_len, d_model = matrix.shape
        pos_enc = np.zeros((seq_len, d_model))
        for pos in range(seq_len):
            for i in range(0, d_model, 2):
                angle = pos / (10000 ** ((2 * i) / d_model))
                pos_enc[pos, i] = np.sin(angle)
                if i + 1 < d_model:
                    pos_enc[pos, i + 1] = np.cos(angle)
        return pos_enc

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def layer_normalization(self, x, eps=1e-6):
        mean = np.mean(x, axis=1, keepdims=True)
        std = np.std(x, axis=1, keepdims=True)
        D = x.shape[1]
        self.norm_stats.append((x.copy(), mean, D, std))
        return (x - mean) / (std + eps)

    def single_head_attention(self, Q, K, V):
        scores = np.dot(Q, K.T) / np.sqrt(self.d_k)
        attn_weights = self.softmax(scores)
        return np.dot(attn_weights, V),attn_weights

    def masked_multihead_attention(self, Q_input, K_input, V_input):
        Q_input += self.positional_encoding(Q_input)
        outputs = []
        for i in range(self.num_heads):
            Q = np.dot(Q_input, self.wq_masked[i])
            K = np.dot(K_input, self.wk_masked[i])
            V = np.dot(V_input, self.wv_masked[i])
            head_output,attnweight = self.single_head_attention(Q, K, V)
            self.maskquery.append(Q)
            self.maskkey.append(K)
            self.maskvalue.append(V)
            self.maskattn.append(attnweight)
            outputs.append(head_output)
        concatenated = np.concatenate(outputs, axis=1)
        residual = Q_input + concatenated
        return self.layer_normalization(residual)

    def cross_multihead_attention(self, Q_input, K_input, V_input):
        Q_input += self.positional_encoding(Q_input)
        outputs = []
        for i in range(self.num_heads):
            Q = np.dot(Q_input, self.wq_cross[i])
            K = np.dot(K_input, self.wk_cross[i])
            V = np.dot(V_input, self.wv_cross[i])
            head_output,attnweight = self.single_head_attention(Q, K, V)
            self.crossquery.append(Q)
            self.crosskey.append(K)
            self.crossvalue.append(V)
            self.crossattn.append(attnweight)
            outputs.append(head_output)
        concatenated = np.concatenate(outputs, axis=1)
        residual = Q_input + concatenated
        return self.layer_normalization(residual)

    def feed_forward_network(self, x):
        self.ann.add(2048, "relu", inputshape=x.shape)
        self.ann.add(self.d_model, "linear")
        ffn_output = self.ann.forward(x)
        residual = x + ffn_output
        return self.layer_normalization(residual)
    def layernorm_backward(self, dy, x, mean, D, sigma, eps=1e-6):
        x_centered = x - mean
        term1 = dy / (sigma + eps)
        mean_dy = np.mean(dy, axis=1, keepdims=True)
        mean_dy_xmu = np.mean(dy * x_centered, axis=1, keepdims=True)
        dx = (term1 -
              mean_dy / (sigma + eps) -
              x_centered * mean_dy_xmu / ((sigma + eps) ** 3))
        return dx

    def decoderforward(self):
        x = self.masked_multihead_attention(self.output_matrix,
                                            self.output_matrix,
                                            self.output_matrix)
        x = self.cross_multihead_attention(x, self.encoder_output, self.encoder_output)
        return self.feed_forward_network(x)

    def crossattentionbackward(self, delta):
       total_dx = np.zeros_like(self.output_matrix) 
       total_dy = np.zeros_like(self.encoder_output)
       for h in range(self.num_heads):
        dels = delta[:, h * self.d_k : (h + 1) * self.d_k]  
        Q = self.crossquery[h] 
        K = self.crosskey[h]
        V = self.crossvalue[h]
        A = self.crossattn[h] 

        dV = np.dot(A.T, dels)
        dwv = np.dot(dV, self.encoder_output.T) 

        d_scores = np.dot(dels, V.T)
        softmax_grad = np.diagflat(A) - np.dot(A, A.T)
        d_attn_scores = np.dot(softmax_grad, d_scores) / np.sqrt(self.d_k)

        dQ = np.dot(d_attn_scores, K)
        dK = np.dot(d_attn_scores.T, Q)

        dwq = np.dot(dQ, self.output_matrix.T)
        dwk = np.dot(dK, self.encoder_output.T)
        dx = np.dot(dQ, self.wq_cross[h])  
        dy = np.dot(dK,self.wk_cross[h])+np.dot(dV,self.wv_cross[h])
        total_dx += dx
        total_dy += dy
        self.optimizer(self.wq_cross[h], dwq, learning_rate=0.001)
        self.optimizer(self.wk_cross[h], dwk, learning_rate=0.001)
        self.optimizer(self.wv_cross[h], dwv, learning_rate=0.001)
       return total_dx,total_dy
    def multiheadattentionbackward(self, delta):
    for h in range(self.num_heads):
        dels = delta[:, h * self.d_k : (h + 1) * self.d_k]  

        Q = self.maskquery[h]       
        K = self.maskkey[h]        
        V = self.maskvalue[h]       
        A = self.maskattn[h]        
        dV = np.dot(A.T, dels)  
        dwv = np.dot(dV.T, self.encoder_output)  

        d_scores = np.dot(dels, V.T)
        softmax_grad = np.diagflat(A) - np.dot(A, A.T)  
        d_attn_scores = np.dot(softmax_grad, d_scores) / np.sqrt(self.d_k)  
        dQ = np.dot(d_attn_scores, K)  
        dK = np.dot(d_attn_scores.T, Q) 

        dwq = np.dot(dQ.T, self.encoder_output)
        dwk = np.dot(dK.T, self.encoder_output)  
        dx = np.dot(dQ, self.wq_masked[h]) + np.dot(dK, self.wk_masked[h]) + np.dot(dV, self.wv_masked[h])
        self.optimizer(self.wq_masked[h], dwq, learning_rate=0.001)
        self.optimizer(self.wk_masked[h], dwk, learning_rate=0.001)
        self.optimizer(self.wv_masked[h], dwv, learning_rate=0.001)

    def backward(self, delta,normstats):
        delta=self.layernorm_backward(delta,normstats[-1])
        delta=self.ann.backprop_from_output(delta)
        delta=self.layernorm_backward(delta,normstats[-2])
        deltadecoder,deltaencoder=self.crossattentionbackward(delta)
        delta=self.layernorm_backward(deltadecoder,normstats[-3])
        delta=self.multiheadattentionbackward(delta)
        return deltaencoder,deltadecoder
class encoder_decoder():
    def __init__(self, input_matrix, output_matrix):
        self.input_matrix = input_matrix
        self.output_matrix = output_matrix
        self.encoder_layers = []
        self.decoder_layers = []

    def softmax(self, x):
        exp = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp / np.sum(exp, axis=-1, keepdims=True)

    def cross_entropy(self, probs, target):
        eps = 1e-9
        return -np.sum(np.log(probs[np.arange(len(target)), target] + eps)) / len(target)

    def forwardtraining(self, targettokens, vocab_size):
        x = self.input_matrix

        for i in range(6):
            encoder = TransformerEncoder(x)
            x = encoder.forward()
            self.encoder_layers.append(encoder)

        y = self.output_matrix
        for j in range(6):
            decoder = TransformerDecoder(y, x)
            y = decoder.decoderforward()
            self.decoder_layers.append(decoder)

        self.output_matrix = y

        linear_weights = np.random.randn(y.shape[1], vocab_size) * np.sqrt(2 / y.shape[1])
        logits = np.dot(y, linear_weights)
        probs = self.softmax(logits)

        loss = self.cross_entropy(probs, targettokens)

        y_true = np.zeros_like(probs)
        y_true[np.arange(len(targettokens)), targettokens] = 1
        dlogits = probs - y_true

        dw = np.dot(y.T, dlogits)
        optimizer = Optimizer()
        optimizer.optimizer('adam', linear_weights, dw, learning_rate=0.001)
        delta = np.dot(dlogits, linear_weights.T)
        deltaencode=np.zeros_like(x)
        for i in range(5,-1,-1):
            decoder=self.decoder_layers[i]
            deltadecoder,deltaencoder=self.decoder_layers[i].backward(delta)
            deltaencoder+=deltaencoder
        for j in range(5,-1,-1):
            encoder=self.encoder_layers[j]
            deltaencoder=self.encoder_layers[j].backward(deltaencoder)

    def predict(self, sentence):
        x = self.input_matrix
        for i in range(6):
            encoder = TransformerEncoder(x)
            x = encoder.forward()

        y = self.output_matrix
        for j in range(6):
            decoder = TransformerDecoder(y, x)
            y = decoder.decoderforward()

        return y






